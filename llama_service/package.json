{
  "name": "llama-service-snt",
  "version": "1.2.4",
  "description": "Client Node.js per LLM locali (Ollama/Llava)",
  "main": "index.js",
  "type": "module",
  "scripts": {
    "test": "node test/test.js",
    "example": "node examples/usage.js"
  },
  "keywords": [
    "llm",
    "ollama",
    "llava",
    "ai",
    "local",
    "client",
    "chat",
    "streaming"
  ],
  "author": "Marco Paglicci <marcopaglicci22@gmail.com>",
  "license": "MIT",
  "repository": {
    "type": "git",
    "url": "git+https://github.com/SNTgpt/llama_service.git"
  },
  "homepage": "https://github.com/SNTgpt/llama_service#readme",
  "bugs": {
    "url": "https://github.com/SNTgpt/llama_service/issues"
  },
  "dependencies": {
    "dotenv": "^16.6.1",
    "node-fetch": "^3.3.2"
  },
  "engines": {
    "node": ">=16.0.0"
  },
  "files": [
    "index.js",
    "README.md",
    "config/",
    "LICENSE"
  ]
}
