# ğŸ§  llama-service-snt

Libreria Node.js **modulare** per interfacciarsi con modelli LLM locali (es. `llava`, `llama3`) tramite Ollama API.  
Supporta autenticazione, streaming real-time, immagini base64 e conversazioni multi-turno.


---

## âœ¨ Caratteristiche v2.0.0

- âœ… **Architettura modulare** - Componenti separati per responsabilitÃ 
- âœ… **Prompt standard** - Invio messaggi semplici
- âœ… **Streaming real-time** - Callback progressivi
- âœ… **Supporto immagini** - Invio immagini base64
- âœ… **Conversazioni multi-turno** - Context management
- âœ… **System prompts** - Personalizzazione comportamento modello
- âœ… **Validazione input** - Controlli automatici
- âœ… **Gestione errori** - Error handling chiaro

---

## ğŸ“¦ Installazione
```bash
npm install llama-service-snt
```

---

## âš™ï¸ Configurazione

### 1. Crea `config/configuration.json`
```json
{
  "api": {
    "host": "http://localhost:11434",
    "key": "sk-your-api-key",
    "timeout": 30000
  },
  "model": {
    "default": "llava"
  },
  "endpoints": {
    "chat": "/api/chat"
  }
}
```

### 2. (Opzionale) Crea `.env` per override
```bash
LLM_API_KEY=sk-your-api-key
LLM_API_HOST=http://localhost:11434
LLM_MODEL=llava
```

**Nota**: I valori in `.env` sovrascrivono quelli in `configuration.json`

---

## ğŸ§ª Utilizzo Base

### Importazione
```javascript
import { LLMClient } from 'llama-service-snt';

const client = new LLMClient('sk-your-api-key', 'llava');
```

---

## ğŸ“š Esempi

### 1ï¸âƒ£ Prompt Semplice
```javascript
const risposta = await client.send("Ciao, chi sei?");
console.log(risposta);
```

### 2ï¸âƒ£ Con System Prompt
```javascript
const risposta = await client.send("Qual Ã¨ la capitale d'Italia?", {
  systemPrompt: "Rispondi in modo conciso, massimo 10 parole"
});
console.log(risposta); // "Roma"
```

### 3ï¸âƒ£ Streaming Real-Time
```javascript
await client.sendStream("Conta da 1 a 5", {
  systemPrompt: "Conta lentamente",
  onChunk: (chunk) => {
    if (!chunk.isComplete) {
      process.stdout.write(chunk.content); // Output progressivo
    } else {
      console.log("\nâœ… Completato!");
    }
  }
});
```

### 4ï¸âƒ£ Streaming Senza Callback
```javascript
const rispostaCompleta = await client.sendStream("Raccontami una storia breve");
console.log(rispostaCompleta); // Ritorna tutta la risposta alla fine
```

### 5ï¸âƒ£ Invio Immagine Base64
```javascript
import { readFileSync } from 'fs';

// Leggi immagine e converti in base64
const imageBuffer = readFileSync('./image.jpg');
const base64Image = imageBuffer.toString('base64');

const risposta = await client.sendWithImage(
  "Cosa vedi in questa immagine?",
  base64Image,
  {
    systemPrompt: "Descrivi l'immagine in dettaglio"
  }
);
console.log(risposta);
```

### 6ï¸âƒ£ Conversazione Multi-Turno
```javascript
const messages = [];

// Turno 1
const turno1 = await client.send("Mi chiamo Marco", {
  systemPrompt: "Sei un assistente che ricorda le informazioni",
  messages: messages
});
messages.push({ role: "user", content: "Mi chiamo Marco" });
messages.push({ role: "assistant", content: turno1 });

// Turno 2
const turno2 = await client.send("Come mi chiamo?", {
  systemPrompt: "Sei un assistente che ricorda le informazioni",
  messages: messages
});
console.log(turno2); // "Ti chiami Marco"
```

---

## ğŸ”§ API Reference

### `new LLMClient(apiKey, model?)`

Crea una nuova istanza del client.

**Parametri:**
- `apiKey` (string) - Chiave API per autenticazione
- `model` (string, opzionale) - Modello da utilizzare (default: da config)

---

### `client.send(message, options?)`

Invia un prompt standard (non-streaming).

**Parametri:**
- `message` (string) - Il messaggio da inviare
- `options` (object, opzionale):
  - `systemPrompt` (string) - Prompt di sistema
  - `messages` (array) - Messaggi precedenti per context

**Ritorna:** `Promise<string>` - Risposta del modello

---

### `client.sendStream(message, options?)`

Invia un prompt con streaming.

**Parametri:**
- `message` (string) - Il messaggio da inviare
- `options` (object, opzionale):
  - `systemPrompt` (string) - Prompt di sistema
  - `messages` (array) - Messaggi precedenti per context
  - `onChunk` (function) - Callback per ogni chunk ricevuto
    - Riceve: `{ content, fullResponse, isComplete }`

**Ritorna:** `Promise<string>` - Risposta completa del modello

---

### `client.sendWithImage(message, base64Image, options?)`

Invia un prompt con immagine base64.

**Parametri:**
- `message` (string) - Il messaggio da inviare
- `base64Image` (string) - Immagine in formato base64
- `options` (object, opzionale):
  - `systemPrompt` (string) - Prompt di sistema
  - `messages` (array) - Messaggi precedenti per context

**Ritorna:** `Promise<string>` - Risposta del modello

---

## ğŸ—ï¸ Architettura
```
src/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ Connection.js       # Gestione connessione
â”‚   â””â”€â”€ RequestBuilder.js   # Costruzione payload API
â”‚
â”œâ”€â”€ handlers/
â”‚   â”œâ”€â”€ PromptHandler.js    # Invio prompt standard
â”‚   â”œâ”€â”€ StreamHandler.js    # Gestione streaming
â”‚   â””â”€â”€ ImageHandler.js     # Gestione immagini
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ ConfigLoader.js     # Caricamento configurazione
â”‚   â””â”€â”€ Validator.js        # Validazione input
â”‚
â””â”€â”€ LLMClient.js            # Facade principale
```

**Principi:**
- âœ… **KISS** - Keep It Simple, Stupid
- âœ… **Single Responsibility** - Un componente = una responsabilitÃ 
- âœ… **Separation of Concerns** - Logica separata per layer

---

## ğŸ§ª Testing
```bash
# Esegui test suite
npm test
```

---

## ğŸ” Sicurezza

- âœ… Validazione API key
- âœ… Validazione input utente
- âœ… Gestione errori di rete
- âš ï¸ **Non committare** `.env` e `config/` (giÃ  in `.gitignore`)

---

## ğŸ“ Changelog

### v2.0.0 (2025-11-10)
- ğŸ‰ **Refactoring completo** - Architettura modulare
- âœ¨ Supporto immagini base64
- âœ¨ Streaming con callback real-time
- âœ¨ Conversazioni multi-turno
- âœ¨ System prompts personalizzabili
- ğŸ› Fix gestione configurazione

### v1.2.4
- Versione monolitica legacy

---


## ğŸ‘¤ Autore

**Marco Paglicci**
- Email: m.paglicci@sntinformatica.it
- GitHub: [@SNTgpt](https://github.com/SNTgpt)

